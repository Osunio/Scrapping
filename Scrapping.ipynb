{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## UDIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "\n",
    "################ LINKS ################\n",
    "\n",
    "################ GRAUS ################\n",
    "\n",
    "# Fijamos la fecha de inicio del crawling.\n",
    "\n",
    "inicio_crawling_udima = datetime.datetime.now()\n",
    "inicio_crawling_udima = (\"%s\" %inicio_crawling_udima)\n",
    "\n",
    "list_of_links_graus = []\n",
    "\n",
    "html_graus = urlopen(\"https://www.udima.es/es/grados.html\") # Insert your URL to extract\n",
    "bsObj_graus = BeautifulSoup(html_graus.read());\n",
    "\n",
    "for link in bsObj_graus.find_all('a'):\n",
    "    list_of_links_graus.append(link.get('href'))\n",
    "    \n",
    "import pandas as pd\n",
    "df_graus =  pd.DataFrame({'links': list_of_links_graus})\n",
    "#df_graus\n",
    "\n",
    "################ MU ################\n",
    "\n",
    "list_of_links_MU = []\n",
    "\n",
    "html_MU = urlopen(\"https://www.udima.es/es/masteres.html\") # Insert your URL to extract\n",
    "bsObj_MU = BeautifulSoup(html_MU.read());\n",
    "\n",
    "for link in bsObj_MU.find_all('a'):\n",
    "    list_of_links_MU.append(link.get('href'))\n",
    "    \n",
    "df_MU = pd.DataFrame({'links': list_of_links_MU})\n",
    "#df_MU\n",
    "\n",
    "################ JUNTAMOS ################\n",
    "\n",
    "df_links = df_graus.append(df_MU, ignore_index=True)\n",
    "#df_links\n",
    "\n",
    "################ DEPURAMOS ################\n",
    "linkdep = df_links.dropna() #Eliminamos N/As\n",
    "linkdep = linkdep[((linkdep['links'].str.contains(\"grado-\")) | (linkdep['links'].str.contains(\"master-\"))) \n",
    "                 & ~(linkdep['links'].str.contains(\"doble\"))] #Subset\n",
    "linkdep = linkdep[~(linkdep['links'].str.contains(\"curso-\"))]\n",
    "linkdep = linkdep.drop_duplicates() # Eliminamos duplicados.\n",
    "linkdep = linkdep.reset_index(drop=True) # Reseteamos índices.\n",
    "#linkdep\n",
    "\n",
    "################ TITLES ################\n",
    "\n",
    "################ GRAUS ################\n",
    "\n",
    "list_of_titles_graus = []\n",
    "\n",
    "html_graus = urlopen(\"https://www.udima.es/es/grados.html\") # Insert your URL to extract\n",
    "bsObj_graus = BeautifulSoup(html_graus.read());\n",
    "\n",
    "for link in bsObj_graus.find_all('a'):\n",
    "    list_of_titles_graus.append((link.text))\n",
    "    \n",
    "df_graus_title = pd.DataFrame({'title': list_of_titles_graus})\n",
    "#df_graus_title\n",
    "\n",
    "################ DEPURAMOS ################\n",
    "\n",
    "graudep = df_graus_title[(df_graus_title['title'].str.contains(\"Grado en\"))]\n",
    "graudep = graudep[~(graudep['title'].str.contains(\"Doble\"))]\n",
    "graudep = graudep[~(graudep['title'].str.contains(\"Blog\"))]\n",
    "graudep = graudep[~(graudep['title'].str.contains(\"Curso\"))]\n",
    "\n",
    "################ MU ################\n",
    "\n",
    "list_of_titles_mu = []\n",
    "\n",
    "html_mu = urlopen(\"https://www.udima.es/es/masteres.html\") # Insert your URL to extract\n",
    "bsObj_mu = BeautifulSoup(html_mu.read());\n",
    "\n",
    "for link in bsObj_mu.find_all('a'):\n",
    "    list_of_titles_mu.append((link.text))\n",
    "    \n",
    "df_mu_title = pd.DataFrame({'title': list_of_titles_mu})\n",
    "#df_mu_title\n",
    "\n",
    "################ DEPURAMOS ################\n",
    "\n",
    "mudep = df_mu_title[(df_mu_title['title'].str.contains(\"Máster Univ\")) | (df_mu_title['title'].str.contains(\"Máster Interuniversitario\"))]\n",
    "mudep = mudep.reset_index(drop=True) # Reseteamos índices.\n",
    "mudep = mudep.drop_duplicates() # Eliminamos duplicados.\n",
    "#mudep\n",
    "\n",
    "################ JUNTAMOS ################\n",
    "\n",
    "titdep = graudep.append(mudep, ignore_index=True)\n",
    "#titdep\n",
    "\n",
    "# Juntamos links con títulos.\n",
    "\n",
    "titdep[\"links\"] = linkdep[\"links\"]\n",
    "titdep['links'] = 'https://www.udima.es' + titdep['links'].astype(str)\n",
    "udima = titdep\n",
    "udima = udima.sort_values(by=['title']) #Ordenamos alfabéticamente.\n",
    "\n",
    "udima.columns = ['programa','url']\n",
    "udima = udima.reset_index(drop=True) # Reseteamos índices.\n",
    "\n",
    "# Añadimos inicio y fin del crawling.\n",
    "\n",
    "fin_crawling_udima = datetime.datetime.now()\n",
    "fin_crawling_udima = (\"%s\" %fin_crawling_udima)\n",
    "\n",
    "udima [\"inicio_crawling\"] = inicio_crawling_udima\n",
    "udima [\"fin_crawling\"] = fin_crawling_udima\n",
    "\n",
    "# Pasamos a date el campo.\n",
    "\n",
    "udima [\"inicio_crawling\"] = pd.to_datetime(udima['inicio_crawling'])\n",
    "udima [\"fin_crawling\"] = pd.to_datetime(udima['fin_crawling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## UNED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "################ LINKS ################\n",
    "\n",
    "################ GRAUS ################\n",
    "\n",
    "# Fijamos la fecha de inicio del crawling.\n",
    "\n",
    "inicio_crawling_uned = datetime.datetime.now()\n",
    "inicio_crawling_uned = (\"%s\" %inicio_crawling_uned)\n",
    "\n",
    "list_of_links_graus = []\n",
    "\n",
    "html_graus = urlopen(\"https://portal.uned.es/portal/page?_pageid=93,1643102&_dad=portal&_schema=PORTAL\") # Insert your URL to extract\n",
    "bsObj_graus = BeautifulSoup(html_graus.read());\n",
    "\n",
    "for link in bsObj_graus.find_all('a'):\n",
    "    list_of_links_graus.append(link.get('href'))\n",
    "    \n",
    "import pandas as pd\n",
    "df_graus = pd.DataFrame({'links': list_of_links_graus})\n",
    "#df_graus\n",
    "\n",
    "################ MU ################\n",
    "\n",
    "list_of_links_MU = []\n",
    "\n",
    "html_MU = urlopen(\"http://portal.uned.es/portal/page?_pageid=93,1113796&_dad=portal&_schema=PORTAL\") # Insert your URL to extract\n",
    "bsObj_MU = BeautifulSoup(html_MU.read());\n",
    "\n",
    "for link in bsObj_MU.find_all('a'):\n",
    "    list_of_links_MU.append(link.get('href'))\n",
    "    \n",
    "df_MU = pd.DataFrame({'links': list_of_links_MU})\n",
    "#df_MU\n",
    "\n",
    "################ JUNTAMOS ################\n",
    "\n",
    "df_links = df_graus.append(df_MU, ignore_index=True)\n",
    "#df_links\n",
    "\n",
    "################ TITLES ################\n",
    "\n",
    "################ GRAUS ################\n",
    "\n",
    "list_of_links_graus2 = []\n",
    "\n",
    "html_graus2 = urlopen(\"https://portal.uned.es/portal/page?_pageid=93,1643102&_dad=portal&_schema=PORTAL\") # Insert your URL to extract\n",
    "bsObj_graus2 = BeautifulSoup(html_graus2.read());\n",
    "\n",
    "for link in bsObj_graus2.find_all('a'):\n",
    "    list_of_links_graus2.append(link.get('title'))\n",
    "    \n",
    "import pandas as pd\n",
    "df_graus2 = pd.DataFrame({'title': list_of_links_graus2})\n",
    "#df_graus2\n",
    "\n",
    "################ MU ################\n",
    "\n",
    "list_of_links_MU2 = []\n",
    "\n",
    "html_MU2 = urlopen(\"http://portal.uned.es/portal/page?_pageid=93,1113796&_dad=portal&_schema=PORTAL\") # Insert your URL to extract\n",
    "bsObj_MU2 = BeautifulSoup(html_MU2.read());\n",
    "\n",
    "for link in bsObj_MU2.find_all('a'):\n",
    "    list_of_links_MU2.append(link.get('title'))\n",
    "    \n",
    "df_MU2 = pd.DataFrame({'title': list_of_links_MU2})\n",
    "#df_MU2\n",
    "\n",
    "# Hay un problema con los títulos (no aparecen para los MU), pero sí que aparecen para abrir el pdf. Hacemos trampilla.\n",
    "\n",
    "titulos = pd.DataFrame(df_MU2[\"title\"]) #Convertimos en df la variable.\n",
    "titulos = titulos.dropna() #Eliminamos N/As\n",
    "titulos = titulos[(titulos['title'].str.contains(\"Abrir pdf guía de M\"))] #Seleccionamos aquellos que tienen el texto.\n",
    "#titulos\n",
    "\n",
    "################ JUNTAMOS ################\n",
    "\n",
    "df_titles = df_graus2.append(df_MU2, ignore_index=True)\n",
    "#df_titles\n",
    "\n",
    "# Creamos el data frame completo.\n",
    "\n",
    "df_titles[\"links\"] = df_links[\"links\"]\n",
    "#df_titles\n",
    "\n",
    "# Depuramos Graus.\n",
    "depurado = df_titles.dropna() #Eliminamos N/As\n",
    "depurado = depurado[(depurado['links'].str.contains(\"/GRADOS/0\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/1\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/2\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/3\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/4\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/5\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/6\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/7\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/8\")) |\n",
    "                    (depurado['links'].str.contains(\"/GRADOS/9\"))]\n",
    "\n",
    "# Depuramos MU.\n",
    "depurado2 = df_titles.dropna() #Eliminamos N/As\n",
    "depurado2 = depurado2[(depurado2['links'].str.contains(\"MASTER2019\"))] #Subset.\n",
    "\n",
    "#Juntamos con titulos, no sin antes resetear índices.\n",
    "titulos = titulos.reset_index(drop=True)\n",
    "depurado2 = depurado2.reset_index(drop=True)\n",
    "depurado2 = depurado2.drop([\"title\"], axis=1)\n",
    "depurado2 = pd.concat([depurado2, titulos], axis=1)\n",
    "\n",
    "# Juntamos y reordenamos columnas.\n",
    "uned = depurado.append(depurado2, ignore_index=True)\n",
    "\n",
    "cols = uned.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "uned = uned[cols]\n",
    "\n",
    "# Eliminamos texto inutil y mejoramos url's.\n",
    "\n",
    "uned[\"title\"] = pd.Series(uned[\"title\"]).str.replace(\"Abrir página web del grado\",\"\")\n",
    "uned[\"title\"] = pd.Series(uned[\"title\"]).str.replace(\"Abrir pdf guía de\",\"\")\n",
    "uned[\"title\"] = pd.Series(uned[\"title\"]).str.replace(\"\\(Ventana nueva\\)\",\"\")\n",
    "\n",
    "uned[\"links\"] = pd.Series(uned[\"links\"]).str.replace(\"/pls/\",\"http://portal.uned.es/pls/\")\n",
    "\n",
    "uned = uned.sort_values(by=['title']) #Ordenamos alfabéticamente.\n",
    "uned = uned.drop_duplicates() # Eliminamos duplicados.\n",
    "uned = uned.reset_index(drop=True) # Reseteamos índices.\n",
    "uned.columns = ['programa','url']\n",
    "\n",
    "# Añadimos inicio y fin del crawling.\n",
    "\n",
    "fin_crawling_uned = datetime.datetime.now()\n",
    "fin_crawling_uned = (\"%s\" %fin_crawling_uned)\n",
    "\n",
    "uned [\"inicio_crawling\"] = inicio_crawling_uned\n",
    "uned [\"fin_crawling\"] = fin_crawling_uned\n",
    "\n",
    "# Pasamos a date el campo.\n",
    "\n",
    "uned [\"inicio_crawling\"] = pd.to_datetime(uned['inicio_crawling'])\n",
    "uned [\"fin_crawling\"] = pd.to_datetime(uned['fin_crawling'])\n",
    "\n",
    "#uned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## UNIBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fijamos la fecha de inicio del crawling.\n",
    "\n",
    "inicio_crawling_uniba = datetime.datetime.now()\n",
    "inicio_crawling_uniba = (\"%s\" %inicio_crawling_uniba)\n",
    "\n",
    "list_of_links = []\n",
    "\n",
    "html = urlopen(\"https://www.unibarcelona.com/es\") # Insert your URL to extract\n",
    "bsObj = BeautifulSoup(html.read());\n",
    "\n",
    "for link in bsObj.find_all('a'):\n",
    "    list_of_links.append(link.get('href'))\n",
    "    \n",
    "import pandas as pd\n",
    "    \n",
    "df = pd.DataFrame({'links': list_of_links})\n",
    "#df\n",
    "\n",
    "list_of_links2 = []\n",
    "\n",
    "for link in bsObj.find_all('a'):\n",
    "    list_of_links2.append((link.text))\n",
    "    \n",
    "df2 = pd.DataFrame({'titles': list_of_links2})\n",
    "#df2\n",
    "\n",
    "df2[\"links\"] = df[\"links\"]\n",
    "\n",
    "uniba = df2.dropna() #Eliminamos N/As \n",
    "uniba = uniba[(uniba['links'].str.contains(\"masters-oficiales\")) | (uniba['links'].str.contains(\"/grados/\"))] #Subset.\n",
    "uniba = uniba[~(uniba['links'].str.contains(\"https://www.unibarcelona.com/\"))] #Subset.\n",
    "uniba = uniba[(uniba['titles'].str.contains(\"Máster\")) | (uniba['titles'].str.contains(\"Grado\"))] #Subset.\n",
    "uniba = uniba[~(uniba['titles'].str.contains(\"Másters\"))] #Subset.\n",
    "uniba = uniba.sort_values(by=['titles']) #Ordenamos alfabéticamente.\n",
    "uniba = uniba.drop_duplicates() # Eliminamos duplicados.\n",
    "uniba = uniba.reset_index(drop=True) # Reseteamos índices.\n",
    "uniba['links'] = 'https://www.unibarcelona.com' + uniba['links'].astype(str) #Añadimos texto.\n",
    "uniba.columns = ['programa','url']\n",
    "\n",
    "# Añadimos inicio y fin del crawling.\n",
    "\n",
    "fin_crawling_uniba = datetime.datetime.now()\n",
    "fin_crawling_uniba = (\"%s\" %fin_crawling_uniba)\n",
    "\n",
    "uniba [\"inicio_crawling\"] = inicio_crawling_uniba\n",
    "uniba [\"fin_crawling\"] = fin_crawling_uniba\n",
    "\n",
    "# Pasamos a date el campo.\n",
    "\n",
    "uniba [\"inicio_crawling\"] = pd.to_datetime(uniba['inicio_crawling'])\n",
    "uniba [\"fin_crawling\"] = pd.to_datetime(uniba['fin_crawling'])\n",
    "\n",
    "#uniba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## UNIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fijamos la fecha de inicio del crawling.\n",
    "\n",
    "inicio_crawling_unir = datetime.datetime.now()\n",
    "inicio_crawling_unir = (\"%s\" %inicio_crawling_unir)\n",
    "\n",
    "list_of_links = []\n",
    "\n",
    "html = urlopen(\"https://www.unir.net/\") # Insert your URL to extract\n",
    "bsObj = BeautifulSoup(html.read());\n",
    "\n",
    "for link in bsObj.find_all('a'):\n",
    "    list_of_links.append(link.get('href'))\n",
    "    \n",
    "import pandas as pd\n",
    "    \n",
    "df = pd.DataFrame({'links': list_of_links})\n",
    "\n",
    "list_of_links2 = []\n",
    "\n",
    "for link in bsObj.find_all('a'):\n",
    "    list_of_links2.append(link.get('title'))\n",
    "    \n",
    "df2 = pd.DataFrame({'titles': list_of_links2})\n",
    "\n",
    "df2[\"links\"] = df[\"links\"]\n",
    "\n",
    "unir = df2.dropna() #Eliminamos N/As \n",
    "unir = unir[(unir['links'].str.contains(\"/master-\")) | (unir['links'].str.contains(\"/grado-\"))] #Subset.\n",
    "unir = unir.sort_values(by=['titles']) #Ordenamos alfabéticamente.\n",
    "unir = unir.drop_duplicates() # Eliminamos duplicados.\n",
    "unir = unir.reset_index(drop=True) # Reseteamos índices.\n",
    "unir.columns = ['programa','url']\n",
    "\n",
    "# Añadimos inicio y fin del crawling.\n",
    "\n",
    "fin_crawling_unir = datetime.datetime.now()\n",
    "fin_crawling_unir = (\"%s\" %fin_crawling_unir)\n",
    "\n",
    "unir [\"inicio_crawling\"] = inicio_crawling_unir\n",
    "unir [\"fin_crawling\"] = fin_crawling_unir\n",
    "\n",
    "# Pasamos a date el campo.\n",
    "\n",
    "unir [\"inicio_crawling\"] = pd.to_datetime(unir['inicio_crawling'])\n",
    "unir [\"fin_crawling\"] = pd.to_datetime(unir['fin_crawling'])\n",
    "#unir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## VIU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\fponsal\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "################ LINKS ################\n",
    "\n",
    "################ GRAUS ################\n",
    "\n",
    "# Fijamos la fecha de inicio del crawling.\n",
    "\n",
    "inicio_crawling_viu = datetime.datetime.now()\n",
    "inicio_crawling_viu = (\"%s\" %inicio_crawling_viu)\n",
    "\n",
    "list_of_links_graus = []\n",
    "\n",
    "html_graus = urlopen(\"https://www.universidadviu.es/grados-online-viu/\") # Insert your URL to extract\n",
    "bsObj_graus = BeautifulSoup(html_graus.read());\n",
    "\n",
    "for link in bsObj_graus.find_all('a'):\n",
    "    list_of_links_graus.append(link.get('href'))\n",
    "    \n",
    "import pandas as pd\n",
    "df_graus = pd.DataFrame({'links': list_of_links_graus})\n",
    "#df_graus\n",
    "\n",
    "################ MU página 1 ################\n",
    "\n",
    "list_of_links_MU1 = []\n",
    "\n",
    "html_MU1 = urlopen(\"https://www.universidadviu.es/masters-oficiales-online/\") # Insert your URL to extract\n",
    "bsObj_MU1 = BeautifulSoup(html_MU1.read());\n",
    "\n",
    "for link in bsObj_MU1.find_all('a'):\n",
    "    list_of_links_MU1.append(link.get('href'))\n",
    "    \n",
    "df_MU1 = pd.DataFrame({'links': list_of_links_MU1})\n",
    "#df_MU1\n",
    "\n",
    "################ MU página 2 ################\n",
    "\n",
    "list_of_links_MU2 = []\n",
    "\n",
    "html_MU2 = urlopen(\"https://www.universidadviu.es/masters-oficiales-online/?page=1\") # Insert your URL to extract\n",
    "bsObj_MU2 = BeautifulSoup(html_MU2.read());\n",
    "\n",
    "for link in bsObj_MU2.find_all('a'):\n",
    "    list_of_links_MU2.append(link.get('href'))\n",
    "    \n",
    "df_MU2 = pd.DataFrame({'links': list_of_links_MU2})\n",
    "#df_MU2\n",
    "\n",
    "################ JUNTAMOS ################\n",
    "\n",
    "df_links = df_graus.append(df_MU1, ignore_index=True)\n",
    "df_links = df_links.append(df_MU2, ignore_index=True)\n",
    "#df_links\n",
    "\n",
    "################ DEPURAMOS ################\n",
    "\n",
    "linkdep = df_links.dropna() #Eliminamos N/As\n",
    "linkdep = linkdep[((linkdep['links'].str.contains(\"grado-\")) | (linkdep['links'].str.contains(\"master-\")))\n",
    "                 & ~(linkdep['links'].str.contains(\"facebook\")) & ~(linkdep['links'].str.contains(\"twitter\"))\n",
    "                  & ~(linkdep['links'].str.contains(\"linkedin\")) & ~(linkdep['links'].str.contains(\"whatsapp\"))]\n",
    "linkdep = linkdep.drop_duplicates() # Eliminamos duplicados.\n",
    "#linkdep\n",
    "\n",
    "################ TITLES ################\n",
    "\n",
    "################ GRAUS ################\n",
    "\n",
    "list_of_titles_graus = []\n",
    "\n",
    "html_graus = urlopen(\"https://www.universidadviu.es/grados-online-viu/\") # Insert your URL to extract\n",
    "bsObj_graus = BeautifulSoup(html_graus.read());\n",
    "\n",
    "for link in bsObj_graus.find_all('a'):\n",
    "    list_of_titles_graus.append((link.text))\n",
    "    \n",
    "df_graus = pd.DataFrame({'title': list_of_titles_graus})\n",
    "#df_graus\n",
    "\n",
    "# Depuramos para que nos queden solo los titulos de los grados.\n",
    "\n",
    "graudep = df_graus.dropna() #Eliminamos N/As\n",
    "graudep = graudep[(graudep['title'].str.contains(\"Grado en\")) & ~(graudep['title'].str.contains(\"\\n\"))]\n",
    "graudep = graudep.drop_duplicates() # Eliminamos duplicados.\n",
    "#graudep\n",
    "\n",
    "################ MU página 1 ################\n",
    "\n",
    "list_of_titles_MU1 = []\n",
    "\n",
    "html_MU1 = urlopen(\"https://www.universidadviu.es/masteres-online/\") # Insert your URL to extract\n",
    "bsObj_MU1 = BeautifulSoup(html_MU1.read());\n",
    "\n",
    "for link in bsObj_MU1.find_all('a'):\n",
    "    list_of_titles_MU1.append((link.text))\n",
    "    \n",
    "df_MU1 = pd.DataFrame({'title': list_of_titles_MU1})\n",
    "#df_MU1\n",
    "\n",
    "################ MU página 2 ################\n",
    "\n",
    "list_of_titles_MU2 = []\n",
    "\n",
    "html_MU2 = urlopen(\"https://www.universidadviu.es/masteres-online/?page=1\") # Insert your URL to extract\n",
    "bsObj_MU2 = BeautifulSoup(html_MU2.read());\n",
    "\n",
    "for link in bsObj_MU2.find_all('a'):\n",
    "    list_of_titles_MU2.append((link.text))\n",
    "    \n",
    "df_MU2 = pd.DataFrame({'title': list_of_titles_MU2})\n",
    "#df_MU2\n",
    "\n",
    "################ JUNTAMOS LOS MU ################\n",
    "\n",
    "df_titles_mu = df_MU1.append(df_MU2, ignore_index=True)\n",
    "#df_titles_mu\n",
    "\n",
    "# Depuramos para que nos queden solo los titulos de los MU.\n",
    "\n",
    "mudep = df_titles_mu.dropna() #Eliminamos N/As\n",
    "mudep = mudep[((mudep['title'].str.contains(\"Máster en\")) | (mudep['title'].str.contains(\"Máster Universitario\")))\n",
    "              & ~(mudep['title'].str.contains(\"\\n\"))]\n",
    "mudep = mudep.drop_duplicates() # Eliminamos duplicados.\n",
    "#mudep\n",
    "\n",
    "################ JUNTAMOS ################\n",
    "\n",
    "df_titles = graudep.append(mudep, ignore_index=True)\n",
    "#df_titles\n",
    "\n",
    "# Juntamos links con títulos.\n",
    "\n",
    "df_titles = df_titles.reset_index(drop=True) # Reseteamos índices.\n",
    "linkdep = linkdep.reset_index(drop=True) # Reseteamos índices.\n",
    "\n",
    "df_titles[\"links\"] = linkdep[\"links\"]\n",
    "viu = df_titles\n",
    "viu = viu.sort_values(by=['title']) #Ordenamos alfabéticamente.\n",
    "viu.columns = ['programa','url']\n",
    "viu = viu.reset_index(drop=True) # Reseteamos índices.\n",
    "\n",
    "# Añadimos inicio y fin del crawling.\n",
    "\n",
    "fin_crawling_viu = datetime.datetime.now()\n",
    "fin_crawling_viu = (\"%s\" %fin_crawling_viu)\n",
    "\n",
    "viu [\"inicio_crawling\"] = inicio_crawling_viu\n",
    "viu [\"fin_crawling\"] = fin_crawling_viu\n",
    "\n",
    "# Pasamos a date el campo.\n",
    "\n",
    "viu [\"inicio_crawling\"] = pd.to_datetime(viu['inicio_crawling'])\n",
    "viu [\"fin_crawling\"] = pd.to_datetime(viu['fin_crawling'])\n",
    "#viu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parche: Añadimos otros campos que en principio son NA pero que a futuro sacaremos.\n",
    "\n",
    "# Universidad.\n",
    "\n",
    "udima [\"universidad\"] = \"UDIMA\"\n",
    "uned [\"universidad\"] = \"UNED\"\n",
    "uniba [\"universidad\"] = \"UNIBA\"\n",
    "unir [\"universidad\"] = \"UNIR\"\n",
    "viu [\"universidad\"] = \"VIU\"\n",
    "\n",
    "# Ordenamos para que la fecha quede primero.\n",
    "\n",
    "udima = udima[['inicio_crawling',\"fin_crawling\", \"universidad\", 'programa', 'url']]\n",
    "uned = uned[['inicio_crawling',\"fin_crawling\", \"universidad\", 'programa', 'url']]\n",
    "uniba = uniba[['inicio_crawling',\"fin_crawling\", \"universidad\", 'programa', 'url']]\n",
    "unir = unir[['inicio_crawling',\"fin_crawling\", \"universidad\", 'programa', 'url']]\n",
    "viu = viu[['inicio_crawling',\"fin_crawling\", \"universidad\", 'programa', 'url']]\n",
    "\n",
    "# CCAA.\n",
    "\n",
    "udima [\"ccaa\"] = \"Online\"\n",
    "uned [\"ccaa\"] = \"Online\"\n",
    "uniba [\"ccaa\"] = \"Online\"\n",
    "unir [\"ccaa\"] = \"Online\"\n",
    "viu [\"ccaa\"] = \"Online\"\n",
    "\n",
    "# Provincia.\n",
    "\n",
    "udima [\"provincia\"] = \"Online\"\n",
    "uned [\"provincia\"] = \"Online\"\n",
    "uniba [\"provincia\"] = \"Online\"\n",
    "unir [\"provincia\"] = \"Online\"\n",
    "viu [\"provincia\"] = \"Online\"\n",
    "\n",
    "# Titularidad.\n",
    "\n",
    "udima [\"titularidad\"] = \"Privada\"\n",
    "uned [\"titularidad\"] = \"Pública\"\n",
    "uniba [\"titularidad\"] = \"Privada\"\n",
    "unir [\"titularidad\"] = \"Privada\"\n",
    "viu [\"titularidad\"] = \"Privada\"\n",
    "\n",
    "# Tipo de curso.\n",
    "\n",
    "udima.loc[udima['programa'].str.contains('Máster'), 'tipo_de_curso'] = 'Máster'\n",
    "udima.loc[udima['programa'].str.contains('Grado'), 'tipo_de_curso'] = 'Grado'\n",
    "\n",
    "uned.loc[uned['programa'].str.contains('MÁSTER'), 'tipo_de_curso'] = 'Máster'\n",
    "uned.loc[uned['programa'].str.contains('MASTER'), 'tipo_de_curso'] = 'Máster'\n",
    "uned.loc[uned['programa'].str.contains('GRADO'), 'tipo_de_curso'] = 'Grado'\n",
    "\n",
    "uniba.loc[uniba['programa'].str.contains('Máster'), 'tipo_de_curso'] = 'Máster'\n",
    "uniba.loc[uniba['programa'].str.contains('Grado'), 'tipo_de_curso'] = 'Grado'\n",
    "\n",
    "unir.loc[unir['programa'].str.contains('Máster'), 'tipo_de_curso'] = 'Máster'\n",
    "unir.loc[unir['programa'].str.contains('Grado'), 'tipo_de_curso'] = 'Grado'\n",
    "\n",
    "viu.loc[viu['programa'].str.contains('Máster'), 'tipo_de_curso'] = 'Máster'\n",
    "viu.loc[viu['programa'].str.contains('Grado'), 'tipo_de_curso'] = 'Grado'\n",
    "\n",
    "# Oficial.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "udima [\"oficial\"] = np.nan\n",
    "uned [\"oficial\"] = np.nan\n",
    "uniba [\"oficial\"] = np.nan\n",
    "unir [\"oficial\"] = np.nan\n",
    "viu [\"oficial\"] = np.nan\n",
    "\n",
    "# Área.\n",
    "\n",
    "udima [\"area\"] = np.nan\n",
    "uned [\"area\"] = np.nan\n",
    "uniba [\"area\"] = np.nan\n",
    "unir [\"area\"] = np.nan\n",
    "viu [\"area\"] = np.nan\n",
    "\n",
    "# Metodología.\n",
    "\n",
    "udima [\"metodologia\"] = np.nan\n",
    "uned [\"metodologia\"] = np.nan\n",
    "uniba [\"metodologia\"] = np.nan\n",
    "unir [\"metodologia\"] = np.nan\n",
    "viu [\"metodologia\"] = np.nan\n",
    "\n",
    "# Créditos.\n",
    "\n",
    "udima [\"creditos\"] = np.nan\n",
    "uned [\"creditos\"] = np.nan\n",
    "uniba [\"creditos\"] = np.nan\n",
    "unir [\"creditos\"] = np.nan\n",
    "viu [\"creditos\"] = np.nan\n",
    "\n",
    "# Tipo de créditos.\n",
    "\n",
    "udima [\"tipo_de_creditos\"] = np.nan\n",
    "uned [\"tipo_de_creditos\"] = np.nan\n",
    "uniba [\"tipo_de_creditos\"] = np.nan\n",
    "unir [\"tipo_de_creditos\"] = np.nan\n",
    "viu [\"tipo_de_creditos\"] = np.nan\n",
    "\n",
    "# Horas.\n",
    "\n",
    "udima [\"horas\"] = np.nan\n",
    "uned [\"horas\"] = np.nan\n",
    "uniba [\"horas\"] = np.nan\n",
    "unir [\"horas\"] = np.nan\n",
    "viu [\"horas\"] = np.nan\n",
    "\n",
    "# Precio total.\n",
    "\n",
    "udima [\"precio_total\"] = np.nan\n",
    "uned [\"precio_total\"] = np.nan\n",
    "uniba [\"precio_total\"] = np.nan\n",
    "unir [\"precio_total\"] = np.nan\n",
    "viu [\"precio_total\"] = np.nan\n",
    "\n",
    "# Precio crédito.\n",
    "\n",
    "udima [\"precio_credito\"] = udima [\"precio_total\"] / udima [\"creditos\"]\n",
    "uned [\"precio_credito\"] = uned [\"precio_total\"] / uned [\"creditos\"]\n",
    "uniba [\"precio_credito\"] = uniba [\"precio_total\"] / uniba [\"creditos\"]\n",
    "unir [\"precio_credito\"] = unir [\"precio_total\"] / unir [\"creditos\"]\n",
    "viu [\"precio_credito\"] = viu [\"precio_total\"] / viu [\"creditos\"]\n",
    "\n",
    "# Descuento.\n",
    "\n",
    "udima [\"descuento\"] = np.nan\n",
    "uned [\"descuento\"] = np.nan\n",
    "uniba [\"descuento\"] = np.nan\n",
    "unir [\"descuento\"] = np.nan\n",
    "viu [\"descuento\"] = np.nan\n",
    "\n",
    "# Plazas.\n",
    "\n",
    "udima [\"plazas\"] = np.nan\n",
    "uned [\"plazas\"] = np.nan\n",
    "uniba [\"plazas\"] = np.nan\n",
    "unir [\"plazas\"] = np.nan\n",
    "viu [\"plazas\"] = np.nan\n",
    "\n",
    "# Fecha de inicio.\n",
    "\n",
    "udima [\"fecha_inicio\"] = np.nan\n",
    "uned [\"fecha_inicio\"] = np.nan\n",
    "uniba [\"fecha_inicio\"] = np.nan\n",
    "unir [\"fecha_inicio\"] = np.nan\n",
    "viu [\"fecha_inicio\"] = np.nan\n",
    "\n",
    "# Fecha fin.\n",
    "\n",
    "udima [\"fecha_fin\"] = np.nan\n",
    "uned [\"fecha_fin\"] = np.nan\n",
    "uniba [\"fecha_fin\"] = np.nan\n",
    "unir [\"fecha_fin\"] = np.nan\n",
    "viu [\"fecha_fin\"] = np.nan\n",
    "\n",
    "# Duración.\n",
    "\n",
    "udima [\"duracion\"] = np.nan\n",
    "uned [\"duracion\"] = np.nan\n",
    "uniba [\"duracion\"] = np.nan\n",
    "unir [\"duracion\"] = np.nan\n",
    "viu [\"duracion\"] = np.nan\n",
    "\n",
    "# Edición.\n",
    "\n",
    "udima [\"edicion\"] = np.nan\n",
    "uned [\"edicion\"] = np.nan\n",
    "uniba [\"edicion\"] = np.nan\n",
    "unir [\"edicion\"] = np.nan\n",
    "viu [\"edicion\"] = np.nan\n",
    "\n",
    "# Observaciones.\n",
    "\n",
    "udima [\"observaciones\"] = \"\"\n",
    "uned [\"observaciones\"] = \"\"\n",
    "uniba [\"observaciones\"] = \"\"\n",
    "unir [\"observaciones\"] = \"\"\n",
    "viu [\"observaciones\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Query.\n",
    "\n",
    "project_id = \"url-compe\"\n",
    "pd.io.gbq.to_gbq(udima, \"udima.udima\", project_id, if_exists=\"append\") #Se pueden guardar los df.\n",
    "pd.io.gbq.to_gbq(uned, \"uned.uned\", project_id, if_exists=\"append\") #Se pueden guardar los df.\n",
    "pd.io.gbq.to_gbq(uniba, \"uniba.uniba\", project_id, if_exists=\"append\") #Se pueden guardar los df.\n",
    "pd.io.gbq.to_gbq(unir, \"unir.unir\", project_id, if_exists=\"append\") #Se pueden guardar los df.\n",
    "pd.io.gbq.to_gbq(viu, \"viu.viu\", project_id, if_exists=\"append\") #Se pueden guardar los df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer = pd.ExcelWriter('Output scrapping.xlsx')\n",
    "#udima.to_excel(writer,'UDIMA',index=False)\n",
    "#uned.to_excel(writer,'UNED',index=False)\n",
    "#uniba.to_excel(writer,'UNIBA',index=False)\n",
    "#unir.to_excel(writer,'UNIR',index=False)\n",
    "#viu.to_excel(writer,'VIU',index=False)\n",
    "#writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
